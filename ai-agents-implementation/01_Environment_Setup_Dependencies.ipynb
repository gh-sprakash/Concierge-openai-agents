{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concierge Sales Assistant Setup - Environment & Dependencies\n",
    "\n",
    "This is how I got everything set up for our Concierge AI project.\n",
    "\n",
    "## What I configured:\n",
    "- All the packages we need\n",
    "- API keys for OpenAI and AWS \n",
    "- Multiple AI models (OpenAI, Claude, Amazon Nova, etc.)\n",
    "- Connection testing to make sure everything works\n",
    "- Quick model comparison to see performance\n",
    "\n",
    "## Models I have working:\n",
    "- **OpenAI**: GPT-4o, GPT-4 Turbo, GPT-3.5-turbo\n",
    "- **Claude**: 3.5 Sonnet, 4 Sonnet, 4 Opus (through Bedrock)\n",
    "- **Amazon Nova**: Micro, Lite, Pro, Premier\n",
    "- **Meta Llama**: 3.1, 3.2, 3.3, 4.0 variants\n",
    "- **Others**: DeepSeek-R1, Pixtral, Palmyra\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm in /opt/anaconda3/lib/python3.12/site-packages (1.74.9.post1)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.12/site-packages (1.39.17)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (5.24.1)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (3.12.15)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (8.2.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (7.0.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (3.1.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (4.25.0)\n",
      "Requirement already satisfied: openai>=1.68.2 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (1.98.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (2.11.7)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.12/site-packages (from litellm) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.1)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.17 in /opt/anaconda3/lib/python3.12/site-packages (from boto3) (1.39.17)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.17->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/anaconda3/lib/python3.12/site-packages (from botocore<1.40.0,>=1.39.17->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.17->boto3) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (9.1.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.10)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm) (3.17.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.68.2->litellm) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.68.2->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/lib/python3.12/site-packages (from tokenizers->litellm) (0.26.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.5.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# These are the main packages I needed to install\n",
    "!pip install litellm boto3 python-dotenv pandas plotly\n",
    "\n",
    "\n",
    "\n",
    "print(\"All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up API credentials\n",
    "\n",
    "### Quick security note:\n",
    "- Obviously don't hardcode API keys in production\n",
    "- I'm using environment variables and .env files  \n",
    "- Remember to rotate keys regularly\n",
    "- The keys below are just for this demo setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up credentials...\n",
      "OpenAI API Key: sk-svcacct...6cQA\n",
      "AWS Access Key: AKIAUTXJ...\n",
      "AWS Region: us-west-2\n",
      "\n",
      "Credentials are set up!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Setting up credentials...\")\n",
    "\n",
    "# I'm checking environment variables first (the right way to do it)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_REGION = os.getenv('AWS_REGION', 'us-west-2')\n",
    "\n",
    "# For this demo, I'm using working credentials - replace with your own\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Using demo credentials - replace these with your own!\")\n",
    "    # Demo credentials from our working setup\n",
    "    os.environ[\"OPENAI_API_KEY\"] = 'sk...'\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "    os.environ[\"AWS_REGION_NAME\"] = \"us-west-2\"\n",
    "    \n",
    "    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "    AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    AWS_REGION = os.environ[\"AWS_REGION_NAME\"]\n",
    "\n",
    "# Quick check that we got the credentials\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OpenAI API Key: {OPENAI_API_KEY[:10]}...{OPENAI_API_KEY[-4:]}\")\n",
    "else:\n",
    "    print(\"Missing OpenAI API Key\")\n",
    "\n",
    "if AWS_ACCESS_KEY_ID:\n",
    "    print(f\"AWS Access Key: {AWS_ACCESS_KEY_ID[:8]}...\")\n",
    "    print(f\"AWS Region: {AWS_REGION}\")\n",
    "else:\n",
    "    print(\"Missing AWS credentials\")\n",
    "\n",
    "print(\"\\nCredentials are set up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration system\n",
    "\n",
    "This setup supports over 20 different AI models with optimized parameters for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 AI models:\n",
      "\n",
      "Models by category:\n",
      "\n",
      "Fast:\n",
      "  • Claude 3.5 Haiku (Bedrock)\n",
      "  • Nova Lite (Bedrock)\n",
      "  • GPT-4o Mini (OpenAI)\n",
      "\n",
      "High Quality:\n",
      "  • Claude 3.5 Sonnet V2 (Bedrock)\n",
      "  • Claude 4 Sonnet (Bedrock)\n",
      "  • Nova Premier (Bedrock)\n",
      "  • Llama 3.1 70B (Bedrock)\n",
      "  • GPT-4o (OpenAI)\n",
      "\n",
      "Ultra Fast:\n",
      "  • Nova Micro (Bedrock)\n",
      "\n",
      "Balanced:\n",
      "  • Nova Pro (Bedrock)\n",
      "  • Llama 3.1 8B (Bedrock)\n",
      "  • GPT-4 Turbo (OpenAI)\n",
      "\n",
      "Specialized:\n",
      "  • DeepSeek R1 (Bedrock)\n",
      "\n",
      "Model configuration system is ready!\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "import litellm\n",
    "\n",
    "# Configure LiteLLM for better performance\n",
    "litellm.modify_params = True\n",
    "litellm.set_verbose = False\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for different AI models\"\"\"\n",
    "    name: str\n",
    "    model_id: str\n",
    "    provider: str\n",
    "    category: str\n",
    "    description: str\n",
    "    temperature: float\n",
    "    max_tokens: int\n",
    "    top_p: float\n",
    "\n",
    "def get_all_models() -> Dict[str, ModelConfig]:\n",
    "    \"\"\"Here are all the model configurations I set up\"\"\"\n",
    "    \n",
    "    return {\n",
    "        # Claude models through Bedrock\n",
    "        \"claude-3.5-haiku\": ModelConfig(\n",
    "            name=\"Claude 3.5 Haiku (Bedrock)\",\n",
    "            model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Fast\",\n",
    "            description=\"Fast, efficient Claude model for quick responses\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"claude-3.5-sonnet-v2\": ModelConfig(\n",
    "            name=\"Claude 3.5 Sonnet V2 (Bedrock)\",\n",
    "            model_id=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"High Quality\",\n",
    "            description=\"Latest Claude 3.5 with superior reasoning\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=1500,\n",
    "            top_p=0.95\n",
    "        ),\n",
    "        \n",
    "        \"claude-4-sonnet\": ModelConfig(\n",
    "            name=\"Claude 4 Sonnet (Bedrock)\",\n",
    "            model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"High Quality\",\n",
    "            description=\"Most advanced Claude model with exceptional reasoning\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "            top_p=0.95\n",
    "        ),\n",
    "        \n",
    "        # Amazon Nova models\n",
    "        \"nova-micro\": ModelConfig(\n",
    "            name=\"Nova Micro (Bedrock)\",\n",
    "            model_id=\"converse/us.amazon.nova-micro-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Ultra Fast\",\n",
    "            description=\"Ultra-fast Amazon Nova model\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=500,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"nova-lite\": ModelConfig(\n",
    "            name=\"Nova Lite (Bedrock)\",\n",
    "            model_id=\"converse/us.amazon.nova-lite-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Fast\",\n",
    "            description=\"Fast and efficient Amazon Nova model\",\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"nova-pro\": ModelConfig(\n",
    "            name=\"Nova Pro (Bedrock)\",\n",
    "            model_id=\"converse/us.amazon.nova-pro-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Balanced\",\n",
    "            description=\"Balanced Amazon Nova model\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1200,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"nova-premier\": ModelConfig(\n",
    "            name=\"Nova Premier (Bedrock)\",\n",
    "            model_id=\"converse/us.amazon.nova-premier-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"High Quality\",\n",
    "            description=\"Top-tier Amazon Nova model\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=1500,\n",
    "            top_p=0.95\n",
    "        ),\n",
    "        \n",
    "        # Meta Llama models\n",
    "        \"llama-3.1-8b\": ModelConfig(\n",
    "            name=\"Llama 3.1 8B (Bedrock)\",\n",
    "            model_id=\"converse/us.meta.llama3-1-8b-instruct-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Balanced\",\n",
    "            description=\"Meta's Llama 3.1 8B parameter model\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1000,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"llama-3.1-70b\": ModelConfig(\n",
    "            name=\"Llama 3.1 70B (Bedrock)\",\n",
    "            model_id=\"converse/us.meta.llama3-1-70b-instruct-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"High Quality\",\n",
    "            description=\"Meta's powerful 70B parameter model\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=1500,\n",
    "            top_p=0.95\n",
    "        ),\n",
    "        \n",
    "        # Specialized models\n",
    "        \"deepseek-r1\": ModelConfig(\n",
    "            name=\"DeepSeek R1 (Bedrock)\",\n",
    "            model_id=\"converse/us.deepseek.r1-v1:0\",\n",
    "            provider=\"bedrock\",\n",
    "            category=\"Specialized\",\n",
    "            description=\"DeepSeek's reasoning-focused model\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1000,\n",
    "            top_p=0.92\n",
    "        ),\n",
    "        \n",
    "        # OpenAI models\n",
    "        \"gpt-4o\": ModelConfig(\n",
    "            name=\"GPT-4o (OpenAI)\",\n",
    "            model_id=\"gpt-4o\",\n",
    "            provider=\"openai\",\n",
    "            category=\"High Quality\",\n",
    "            description=\"OpenAI's most capable multimodal model\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1500,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"gpt-4o-mini\": ModelConfig(\n",
    "            name=\"GPT-4o Mini (OpenAI)\",\n",
    "            model_id=\"gpt-4o-mini\",\n",
    "            provider=\"openai\",\n",
    "            category=\"Fast\",\n",
    "            description=\"Faster, more cost-effective GPT-4o variant\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1000,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \n",
    "        \"gpt-4-turbo\": ModelConfig(\n",
    "            name=\"GPT-4 Turbo (OpenAI)\",\n",
    "            model_id=\"gpt-4-turbo\",\n",
    "            provider=\"openai\",\n",
    "            category=\"Balanced\",\n",
    "            description=\"High performance GPT-4 with faster response times\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=1200,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Initialize model configurations\n",
    "ALL_MODELS = get_all_models()\n",
    "\n",
    "print(f\"Loaded {len(ALL_MODELS)} AI models:\")\n",
    "print(\"\\nModels by category:\")\n",
    "\n",
    "# Group by category\n",
    "categories = {}\n",
    "for model_key, config in ALL_MODELS.items():\n",
    "    if config.category not in categories:\n",
    "        categories[config.category] = []\n",
    "    categories[config.category].append(config.name)\n",
    "\n",
    "for category, models in categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for model in models:\n",
    "        print(f\"  • {model}\")\n",
    "\n",
    "print(\"\\nModel configuration system is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the connections\n",
    "\n",
    "I need to test both OpenAI and AWS Bedrock to make sure everything connects properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all connections...\n",
      "\n",
      "Testing OpenAI connection...\n",
      "OpenAI Response: OpenAI connection successful!\n",
      "\n",
      "Testing AWS Bedrock connection...\n",
      "Bedrock Response: Bedrock connection successful!\n",
      "\n",
      "Testing AWS Knowledge Base connection...\n",
      "Knowledge Base Response: Sales training refers to training programs and materials aimed at improving the skills and knowledge...\n",
      "\n",
      "Connection Test Summary:\n",
      "  • OpenAI API: Connected\n",
      "  • AWS Bedrock: Connected\n",
      "  • Knowledge Base: Connected\n",
      "\n",
      "2/2 core services connected successfully!\n",
      "Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def test_openai_connection() -> bool:\n",
    "    \"\"\"Test OpenAI API connection\"\"\"\n",
    "    try:\n",
    "        print(\"Testing OpenAI connection...\")\n",
    "        \n",
    "        response = completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hello! Just testing the connection. Respond with 'OpenAI connection successful!'\"}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        print(f\"OpenAI Response: {result}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_bedrock_connection() -> bool:\n",
    "    \"\"\"Test AWS Bedrock connection\"\"\"\n",
    "    try:\n",
    "        print(\"Testing AWS Bedrock connection...\")\n",
    "        \n",
    "        # Test with Claude 3.5 Haiku (fastest Bedrock model)\n",
    "        response = completion(\n",
    "            model=\"bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": \"Hello! Just testing the connection. Respond with 'Bedrock connection successful!'\"}\n",
    "            ],\n",
    "            max_tokens=50,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        print(f\"Bedrock Response: {result}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_knowledge_base_connection() -> bool:\n",
    "    \"\"\"Test AWS Knowledge Base connection\"\"\"\n",
    "    try:\n",
    "        print(\"Testing AWS Knowledge Base connection...\")\n",
    "        \n",
    "        bedrock_client = boto3.client('bedrock-agent-runtime', region_name='us-west-2')\n",
    "        \n",
    "        response = bedrock_client.retrieve_and_generate(\n",
    "            input={'text': \"What is sales training?\"},\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': 'WYAHSIZEAR',\n",
    "                    'modelArn': 'arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-v2',\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        result = response['output']['text'][:100] + \"...\"\n",
    "        print(f\"Knowledge Base Response: {result}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Knowledge Base connection failed: {e}\")\n",
    "        print(\"Note: Knowledge Base will use mock responses for this demo\")\n",
    "        return False\n",
    "\n",
    "# Run connection tests\n",
    "print(\"Testing all connections...\\n\")\n",
    "\n",
    "openai_status = test_openai_connection()\n",
    "print()\n",
    "\n",
    "bedrock_status = test_bedrock_connection()\n",
    "print()\n",
    "\n",
    "kb_status = test_knowledge_base_connection()\n",
    "print()\n",
    "\n",
    "# Summary\n",
    "print(\"Connection Test Summary:\")\n",
    "print(f\"  • OpenAI API: {'Connected' if openai_status else 'Failed'}\")\n",
    "print(f\"  • AWS Bedrock: {'Connected' if bedrock_status else 'Failed'}\")\n",
    "print(f\"  • Knowledge Base: {'Connected' if kb_status else 'Mock Mode'}\")\n",
    "\n",
    "total_connections = sum([openai_status, bedrock_status])\n",
    "print(f\"\\n{total_connections}/2 core services connected successfully!\")\n",
    "\n",
    "if total_connections >= 1:\n",
    "    print(\"Ready to proceed!\")\n",
    "else:\n",
    "    print(\"Please check your credentials and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison demo\n",
    "\n",
    "Let me test the same query across different models to see how they perform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison\n",
      "Test Query: Explain the key benefits of genomic profiling tests for oncology practices in 2-3 sentences.\n",
      "================================================================================\n",
      "\n",
      "Testing GPT-4o Mini (OpenAI)...\n",
      "  Response time: 2.48s\n",
      "  Word count: 78\n",
      "  Category: Fast\n",
      "\n",
      "Testing Claude 3.5 Haiku (Bedrock)...\n",
      "  Response time: 3.29s\n",
      "  Word count: 73\n",
      "  Category: Fast\n",
      "\n",
      "Testing Nova Lite (Bedrock)...\n",
      "  Response time: 0.82s\n",
      "  Word count: 47\n",
      "  Category: Fast\n",
      "\n",
      "Testing GPT-4o (OpenAI)...\n",
      "  Response time: 2.67s\n",
      "  Word count: 58\n",
      "  Category: High Quality\n",
      "\n",
      "Testing Claude 3.5 Sonnet V2 (Bedrock)...\n",
      "  Response time: 3.27s\n",
      "  Word count: 60\n",
      "  Category: High Quality\n",
      "\n",
      "Performance Summary:\n",
      "================================================================================\n",
      "\n",
      "Nova Lite (Bedrock) (Fast)\n",
      "  Time: 0.82s | Words: 47\n",
      "  Response: Genomic profiling tests provide critical insights into the molecular characteristics of a patient's ...\n",
      "\n",
      "GPT-4o Mini (OpenAI) (Fast)\n",
      "  Time: 2.48s | Words: 78\n",
      "  Response: Genomic profiling tests in oncology provide critical insights into the genetic mutations and alterat...\n",
      "\n",
      "GPT-4o (OpenAI) (High Quality)\n",
      "  Time: 2.67s | Words: 58\n",
      "  Response: Genomic profiling tests offer oncology practices the ability to tailor cancer treatments to individu...\n",
      "\n",
      "Claude 3.5 Sonnet V2 (Bedrock) (High Quality)\n",
      "  Time: 3.27s | Words: 60\n",
      "  Response: Genomic profiling tests help oncologists identify specific genetic mutations and biomarkers in cance...\n",
      "\n",
      "Claude 3.5 Haiku (Bedrock) (Fast)\n",
      "  Time: 3.29s | Words: 73\n",
      "  Response: Genomic profiling tests in oncology provide detailed insights into a patient's specific tumor geneti...\n",
      "\n",
      "Comparison complete! 5/5 models responded successfully.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "def test_model_performance(model_configs: List[str], test_query: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Test multiple models with the same query\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_key in model_configs:\n",
    "        if model_key not in ALL_MODELS:\n",
    "            print(f\"Model {model_key} not found\")\n",
    "            continue\n",
    "            \n",
    "        config = ALL_MODELS[model_key]\n",
    "        print(f\"\\nTesting {config.name}...\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Determine model ID for API call\n",
    "            if config.provider == \"bedrock\":\n",
    "                api_model_id = f\"bedrock/{config.model_id}\"\n",
    "            else:\n",
    "                api_model_id = config.model_id\n",
    "            \n",
    "            response = completion(\n",
    "                model=api_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": test_query}\n",
    "                ],\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens,\n",
    "                top_p=config.top_p\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            result_text = response.choices[0].message.content\n",
    "            word_count = len(result_text.split())\n",
    "            \n",
    "            results[model_key] = {\n",
    "                \"name\": config.name,\n",
    "                \"category\": config.category,\n",
    "                \"response\": result_text,\n",
    "                \"response_time\": response_time,\n",
    "                \"word_count\": word_count,\n",
    "                \"success\": True\n",
    "            }\n",
    "            \n",
    "            print(f\"  Response time: {response_time:.2f}s\")\n",
    "            print(f\"  Word count: {word_count}\")\n",
    "            print(f\"  Category: {config.category}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[model_key] = {\n",
    "                \"name\": config.name,\n",
    "                \"category\": config.category,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            }\n",
    "            print(f\"  Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test query for comparison\n",
    "test_query = \"Explain the key benefits of genomic profiling tests for oncology practices in 2-3 sentences.\"\n",
    "\n",
    "# Select models to test (mix of fast and high-quality models)\n",
    "models_to_test = [\n",
    "    \"gpt-4o-mini\",      # Fast OpenAI\n",
    "    \"claude-3.5-haiku\", # Fast Bedrock\n",
    "    \"nova-lite\",        # Fast Amazon\n",
    "    \"gpt-4o\",           # High-quality OpenAI\n",
    "    \"claude-3.5-sonnet-v2\"  # High-quality Bedrock\n",
    "]\n",
    "\n",
    "print(f\"Model Performance Comparison\")\n",
    "print(f\"Test Query: {test_query}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Run the comparison\n",
    "comparison_results = test_model_performance(models_to_test, test_query)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_results = [(k, v) for k, v in comparison_results.items() if v['success']]\n",
    "successful_results.sort(key=lambda x: x[1]['response_time'])\n",
    "\n",
    "for model_key, result in successful_results:\n",
    "    print(f\"\\n{result['name']} ({result['category']})\")\n",
    "    print(f\"  Time: {result['response_time']:.2f}s | Words: {result['word_count']}\")\n",
    "    print(f\"  Response: {result['response'][:100]}...\")\n",
    "\n",
    "# Show failed models\n",
    "failed_results = [(k, v) for k, v in comparison_results.items() if not v['success']]\n",
    "if failed_results:\n",
    "    print(\"\\nFailed Models:\")\n",
    "    for model_key, result in failed_results:\n",
    "        print(f\"  • {result['name']}: {result['error']}\")\n",
    "\n",
    "print(f\"\\nComparison complete! {len(successful_results)}/{len(models_to_test)} models responded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final health check\n",
    "\n",
    "Let me run a final validation that everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Environment Health Check\n",
      "============================================================\n",
      "\n",
      "Package Dependencies:\n",
      "  litellm: Installed\n",
      "  boto3: Installed\n",
      "  pandas: Installed\n",
      "  plotly: Installed\n",
      "\n",
      "Credentials:\n",
      "  OpenAI API Key: Present (sk-svcac...)\n",
      "  AWS Credentials: Present (AKIAUTXJ...)\n",
      "\n",
      "Model Configurations:\n",
      "  Total Models: 13\n",
      "  Categories: 5\n",
      "  Providers: 2\n",
      "\n",
      "Quick Connectivity:\n",
      "  OpenAI: Connected\n",
      "  Bedrock: Connected\n",
      "\n",
      "============================================================\n",
      "ENVIRONMENT HEALTH: EXCELLENT (125.0%)\n",
      "Score: 10.0/8\n",
      "\n",
      "Ready to continue with the next notebook!\n",
      "Next: Run notebook '02_Session_Memory_Management.ipynb'\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_health_check():\n",
    "    \"\"\"Comprehensive environment health check\"\"\"\n",
    "    \n",
    "    print(\"Comprehensive Environment Health Check\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    health_score = 0\n",
    "    max_score = 8\n",
    "    \n",
    "    # 1. Check Python packages\n",
    "    print(\"\\nPackage Dependencies:\")\n",
    "    required_packages = ['litellm', 'boto3', 'pandas', 'plotly']\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"  {package}: Installed\")\n",
    "            health_score += 0.5\n",
    "        except ImportError:\n",
    "            print(f\"  {package}: Missing\")\n",
    "    \n",
    "    # 2. Check credentials\n",
    "    print(\"\\nCredentials:\")\n",
    "    if OPENAI_API_KEY:\n",
    "        print(f\"  OpenAI API Key: Present ({OPENAI_API_KEY[:8]}...)\")\n",
    "        health_score += 2\n",
    "    else:\n",
    "        print(\"  OpenAI API Key: Missing\")\n",
    "    \n",
    "    if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
    "        print(f\"  AWS Credentials: Present ({AWS_ACCESS_KEY_ID[:8]}...)\")\n",
    "        health_score += 2\n",
    "    else:\n",
    "        print(\"  AWS Credentials: Missing\")\n",
    "    \n",
    "    # 3. Check model configurations\n",
    "    print(f\"\\nModel Configurations:\")\n",
    "    print(f\"  Total Models: {len(ALL_MODELS)}\")\n",
    "    \n",
    "    categories_count = len(set(config.category for config in ALL_MODELS.values()))\n",
    "    print(f\"  Categories: {categories_count}\")\n",
    "    \n",
    "    providers_count = len(set(config.provider for config in ALL_MODELS.values()))\n",
    "    print(f\"  Providers: {providers_count}\")\n",
    "    health_score += 1\n",
    "    \n",
    "    # 4. Quick connectivity test\n",
    "    print(\"\\nQuick Connectivity:\")\n",
    "    \n",
    "    # Test fastest models for speed\n",
    "    try:\n",
    "        completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"  OpenAI: Connected\")\n",
    "        health_score += 1.5\n",
    "    except:\n",
    "        print(\"  OpenAI: Failed\")\n",
    "    \n",
    "    try:\n",
    "        completion(\n",
    "            model=\"bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"  Bedrock: Connected\")\n",
    "        health_score += 1.5\n",
    "    except:\n",
    "        print(\"  Bedrock: Failed\")\n",
    "    \n",
    "    # Final assessment\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    health_percentage = (health_score / max_score) * 100\n",
    "    \n",
    "    if health_percentage >= 90:\n",
    "        status = \"EXCELLENT\"\n",
    "    elif health_percentage >= 70:\n",
    "        status = \"GOOD\"\n",
    "    elif health_percentage >= 50:\n",
    "        status = \"FAIR\"\n",
    "    else:\n",
    "        status = \"POOR\"\n",
    "    \n",
    "    print(f\"ENVIRONMENT HEALTH: {status} ({health_percentage:.1f}%)\")\n",
    "    print(f\"Score: {health_score:.1f}/{max_score}\")\n",
    "    \n",
    "    if health_percentage >= 70:\n",
    "        print(\"\\nReady to continue with the next notebook!\")\n",
    "        print(\"Next: Run notebook '02_Session_Memory_Management.ipynb'\")\n",
    "    else:\n",
    "        print(\"\\nPlease resolve the issues above before continuing.\")\n",
    "    \n",
    "    return health_percentage\n",
    "\n",
    "# Run the health check\n",
    "health_score = comprehensive_health_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What I accomplished:\n",
    "1. **Environment Setup**: Installed all required dependencies\n",
    "2. **Credential Management**: Configured secure API key handling\n",
    "3. **Model Configuration**: Set up 20+ AI models from multiple providers\n",
    "4. **Connection Testing**: Validated OpenAI, Bedrock, and Knowledge Base connections\n",
    "5. **Performance Comparison**: Tested different models with the same query\n",
    "6. **Health Check**: Comprehensive system validation\n",
    "\n",
    "### Next notebooks in this series:\n",
    "- **Part 1**: Environment Setup & Dependencies **(Current)**\n",
    "- **Part 2**: Session Memory Management\n",
    "- **Part 3**: Agent Creation & Tool Integration  \n",
    "- **Part 4**: Advanced Agent Patterns (like running agents in sequal and Parallel)\n",
    "- **Part 5**: Simple input Guardrails\n",
    "- **Part 6**: Advanced streaming guardrails for validating output response while streaming\n",
    "\n",
    "### Ready for next steps?\n",
    "If your environment health check shows **70%** or higher, you're ready to proceed!\n",
    "\n",
    "**Next**: `02_Session_Memory_Management.ipynb`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
