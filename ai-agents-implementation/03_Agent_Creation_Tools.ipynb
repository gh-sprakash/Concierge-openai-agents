{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Creation with Multi-Model Support\n",
    "\n",
    "## Here's how I got agents working with both OpenAI and Bedrock models\n",
    "\n",
    "**The challenge**: We need agents that can use both OpenAI and Bedrock Claude models  \n",
    "\n",
    "## What I set up here:\n",
    "- **Multi-Provider Support**: Agents work with OpenAI and Bedrock models\n",
    "- **Unified Interface**: Same tools, different AI brains  \n",
    "- **Tool Integration**: Business functions work across all models\n",
    "\n",
    "## Models I got working:\n",
    "- **OpenAI**: GPT-4o Mini, GPT-4o  \n",
    "- **Claude**: 3.5 Sonnet V2, 4 Sonnet (via Bedrock)\n",
    "- **Amazon Nova**: Lite, Pro (via Bedrock)\n",
    "\n",
    "## Key integration components:\n",
    "- **LiteLLM**: For Bedrock model calls\n",
    "- **Agents SDK**: For OpenAI models\n",
    "- **Unified Interface**: Single API for everything\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up dependencies for multi-model support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents SDK ready!\n",
      "LiteLLM ready for Bedrock!\n",
      "Credentials configured for both OpenAI and Bedrock!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "# Import agents SDK\n",
    "try:\n",
    "    from agents import (\n",
    "        Agent, \n",
    "        Runner, \n",
    "        SQLiteSession,\n",
    "        function_tool, \n",
    "        RunContextWrapper,\n",
    "        ModelSettings,\n",
    "        set_tracing_disabled\n",
    "    )\n",
    "    set_tracing_disabled(True)\n",
    "    AGENTS_AVAILABLE = True\n",
    "    print(\"Agents SDK ready!\")\n",
    "except ImportError:\n",
    "    print(\"Install: pip install openai-agents-sdk\")\n",
    "    AGENTS_AVAILABLE = False\n",
    "\n",
    "# Import LiteLLM for Bedrock\n",
    "try:\n",
    "    import litellm\n",
    "    from litellm import completion\n",
    "    litellm.modify_params = True\n",
    "    litellm.set_verbose = False\n",
    "    LITELLM_AVAILABLE = True\n",
    "    print(\"LiteLLM ready for Bedrock!\")\n",
    "except ImportError:\n",
    "    print(\"Install: pip install litellm\")\n",
    "    LITELLM_AVAILABLE = False\n",
    "\n",
    "# Set up credentials (from our working code)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'sk...'\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"us-west-2\"\n",
    "\n",
    "print(\"Credentials configured for both OpenAI and Bedrock!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-model configuration from our working Streamlit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 models configured:\n",
      "   Fast: GPT-4o Mini (OpenAI)\n",
      "   High Quality: GPT-4o (OpenAI)\n",
      "   High Quality: Claude 3.5 Sonnet V2 (Bedrock)\n",
      "   Premium Quality: Claude 4 Sonnet (Bedrock)\n",
      "   Fast: Nova Lite (Bedrock)\n",
      "   Balanced: Nova Pro (Bedrock)\n",
      "\n",
      "Multi-provider model configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# Multi-Model Configuration (from our working Streamlit app)\n",
    "ALL_MODELS = {\n",
    "    # OpenAI models\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"name\": \"GPT-4o Mini (OpenAI)\",\n",
    "        \"model_id\": \"gpt-4o-mini\",\n",
    "        \"provider\": \"openai\",\n",
    "        \"category\": \"Fast\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    \n",
    "    \"gpt-4o\": {\n",
    "        \"name\": \"GPT-4o (OpenAI)\",\n",
    "        \"model_id\": \"gpt-4o\", \n",
    "        \"provider\": \"openai\",\n",
    "        \"category\": \"High Quality\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1500,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    \n",
    "    # Bedrock Claude models\n",
    "    \"claude-3.5-sonnet-v2\": {\n",
    "        \"name\": \"Claude 3.5 Sonnet V2 (Bedrock)\",\n",
    "        \"model_id\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "        \"provider\": \"bedrock\",\n",
    "        \"category\": \"High Quality\",\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 1500,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    "    \n",
    "    \"claude-4-sonnet\": {\n",
    "        \"name\": \"Claude 4 Sonnet (Bedrock)\",\n",
    "        \"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "        \"provider\": \"bedrock\", \n",
    "        \"category\": \"Premium Quality\",\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    "    \n",
    "    # Amazon Nova models\n",
    "    \"nova-lite\": {\n",
    "        \"name\": \"Nova Lite (Bedrock)\",\n",
    "        \"model_id\": \"converse/us.amazon.nova-lite-v1:0\",\n",
    "        \"provider\": \"bedrock\",\n",
    "        \"category\": \"Fast\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    \n",
    "    \"nova-pro\": {\n",
    "        \"name\": \"Nova Pro (Bedrock)\",\n",
    "        \"model_id\": \"converse/us.amazon.nova-pro-v1:0\",\n",
    "        \"provider\": \"bedrock\",\n",
    "        \"category\": \"Balanced\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 1200,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"{len(ALL_MODELS)} models configured:\")\n",
    "for model_key, config in ALL_MODELS.items():\n",
    "    print(f\"   {config['category']}: {config['name']}\")\n",
    "\n",
    "print(\"\\nMulti-provider model configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business data and tools with tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business tools created with tracking!\n"
     ]
    }
   ],
   "source": [
    "# Business Data\n",
    "SALESFORCE_DATA = {\n",
    "    \"orders\": [\n",
    "        {\"doctor\": \"Dr. Smith\", \"product\": \"Guardant360\", \"quantity\": 3, \"amount\": 7500, \"status\": \"Completed\", \"date\": \"2024-01-15\"},\n",
    "        {\"doctor\": \"Dr. Johnson\", \"product\": \"GuardantOMNI\", \"quantity\": 2, \"amount\": 6400, \"status\": \"Processing\", \"date\": \"2024-01-20\"},\n",
    "        {\"doctor\": \"Dr. Martinez\", \"product\": \"Guardant360\", \"quantity\": 1, \"amount\": 2500, \"status\": \"Completed\", \"date\": \"2024-01-18\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "VEEVA_DATA = {\n",
    "    \"engagements\": [\n",
    "        {\"doctor\": \"Dr. Smith\", \"date\": \"2024-01-22\", \"type\": \"Virtual Meeting\", \"outcome\": \"Positive - Ordered tests\", \"rep\": \"John Smith\"},\n",
    "        {\"doctor\": \"Dr. Johnson\", \"date\": \"2024-01-19\", \"type\": \"Email\", \"outcome\": \"Interested in pricing\", \"rep\": \"Sarah Chen\"},\n",
    "        {\"doctor\": \"Dr. Martinez\", \"date\": \"2024-01-17\", \"type\": \"In-Person\", \"outcome\": \"Discussed turnaround times\", \"rep\": \"Mike Davis\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "class SalesContext:\n",
    "    def __init__(self, user_name: str = \"Sales Rep\", territory: str = \"Northeast\"):\n",
    "        self.user_name = user_name\n",
    "        self.territory = territory\n",
    "        self.salesforce_data = SALESFORCE_DATA\n",
    "        self.veeva_data = VEEVA_DATA\n",
    "\n",
    "# Tool call tracking\n",
    "TOOL_CALLS_LOG = []\n",
    "\n",
    "@function_tool\n",
    "async def query_salesforce(ctx: RunContextWrapper[SalesContext], doctor_name: Optional[str] = None) -> str:\n",
    "    \"\"\"Query Salesforce for doctor orders and sales data\"\"\"\n",
    "    TOOL_CALLS_LOG.append(\"query_salesforce\")\n",
    "    print(f\"[TOOL CALLED] query_salesforce: doctor={doctor_name}\")\n",
    "    \n",
    "    orders = ctx.context.salesforce_data[\"orders\"]\n",
    "    \n",
    "    if doctor_name:\n",
    "        filtered_orders = [o for o in orders if doctor_name.lower() in o[\"doctor\"].lower()]\n",
    "        if filtered_orders:\n",
    "            total_amount = sum(o[\"amount\"] for o in filtered_orders)\n",
    "            total_quantity = sum(o[\"quantity\"] for o in filtered_orders)\n",
    "            result = f\"{doctor_name} has {len(filtered_orders)} orders totaling ${total_amount:,} for {total_quantity} tests. \"\n",
    "            result += f\"Recent: {filtered_orders[-1]['product']} ({filtered_orders[-1]['status']})\"\n",
    "            return result\n",
    "        else:\n",
    "            return f\"No orders found for {doctor_name}\"\n",
    "    else:\n",
    "        total_orders = len(orders)\n",
    "        total_revenue = sum(o[\"amount\"] for o in orders)\n",
    "        return f\"Total: {total_orders} orders generating ${total_revenue:,} in revenue\"\n",
    "\n",
    "@function_tool\n",
    "async def query_veeva(ctx: RunContextWrapper[SalesContext], doctor_name: str) -> str:\n",
    "    \"\"\"Query Veeva for doctor engagement history\"\"\"\n",
    "    TOOL_CALLS_LOG.append(\"query_veeva\")\n",
    "    print(f\"[TOOL CALLED] query_veeva: doctor={doctor_name}\")\n",
    "    \n",
    "    engagements = ctx.context.veeva_data[\"engagements\"]\n",
    "    doctor_engagements = [e for e in engagements if doctor_name.lower() in e[\"doctor\"].lower()]\n",
    "    \n",
    "    if doctor_engagements:\n",
    "        latest = doctor_engagements[-1]\n",
    "        return f\"{doctor_name} - Last engagement: {latest['date']} ({latest['type']}) with {latest['rep']}. Outcome: {latest['outcome']}\"\n",
    "    else:\n",
    "        return f\"No engagement records found for {doctor_name}\"\n",
    "\n",
    "print(\"Business tools created with tracking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified model interface for OpenAI + Bedrock\n",
    "\n",
    "The key insight: Create a unified interface that can call both OpenAI Agents SDK and Bedrock via LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified model interface created!\n",
      "   OpenAI models: Via Agents SDK\n",
      "   Bedrock models: Via LiteLLM\n"
     ]
    }
   ],
   "source": [
    "class UnifiedModelInterface:\n",
    "    \"\"\"Unified interface for both OpenAI Agents SDK and Bedrock LiteLLM calls\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_agents = {}  # Store OpenAI agents\n",
    "        \n",
    "    def create_openai_agent(self, model_key: str) -> Agent:\n",
    "        \"\"\"Create OpenAI agent using Agents SDK\"\"\"\n",
    "        config = ALL_MODELS[model_key]\n",
    "        \n",
    "        return Agent(\n",
    "            name=f\"Sales Assistant ({config['name']})\",\n",
    "            instructions=\"\"\"You are a sales assistant for a genomics company.\n",
    "            \n",
    "            Available tools:\n",
    "            • query_salesforce: Get order data, revenue, test information\n",
    "            • query_veeva: Get engagement history, last interactions\n",
    "            \n",
    "            Usage rules:\n",
    "            • When asked about orders/tests/revenue → use query_salesforce\n",
    "            • When asked about meetings/engagements/interactions → use query_veeva\n",
    "            • Always use tools to get accurate data\n",
    "            • Provide specific, helpful responses\n",
    "            \"\"\",\n",
    "            tools=[query_salesforce, query_veeva],\n",
    "            model=config[\"model_id\"],\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=config[\"temperature\"],\n",
    "                max_tokens=config[\"max_tokens\"]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    async def call_bedrock_model(self, model_key: str, query: str, context: SalesContext) -> Dict[str, Any]:\n",
    "        \"\"\"Call Bedrock model using LiteLLM (based on our working code)\"\"\"\n",
    "        config = ALL_MODELS[model_key]\n",
    "        \n",
    "        # Simulate tool routing logic (simplified)\n",
    "        tools_used = []\n",
    "        tool_responses = []\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Simple tool routing\n",
    "        if any(keyword in query_lower for keyword in [\"order\", \"test\", \"revenue\", \"sales\"]):\n",
    "            # Extract doctor name if mentioned\n",
    "            doctor_name = None\n",
    "            for order in SALESFORCE_DATA[\"orders\"]:\n",
    "                if order[\"doctor\"].lower() in query_lower:\n",
    "                    doctor_name = order[\"doctor\"]\n",
    "                    break\n",
    "            \n",
    "            TOOL_CALLS_LOG.append(\"query_salesforce\")\n",
    "            print(f\"[TOOL CALLED] query_salesforce: doctor={doctor_name}\")\n",
    "            \n",
    "            if doctor_name:\n",
    "                filtered_orders = [o for o in SALESFORCE_DATA[\"orders\"] if doctor_name.lower() in o[\"doctor\"].lower()]\n",
    "                if filtered_orders:\n",
    "                    total_amount = sum(o[\"amount\"] for o in filtered_orders)\n",
    "                    total_quantity = sum(o[\"quantity\"] for o in filtered_orders)\n",
    "                    tool_response = f\"{doctor_name} has {len(filtered_orders)} orders totaling ${total_amount:,} for {total_quantity} tests.\"\n",
    "                else:\n",
    "                    tool_response = f\"No orders found for {doctor_name}\"\n",
    "            else:\n",
    "                total_orders = len(SALESFORCE_DATA[\"orders\"])\n",
    "                total_revenue = sum(o[\"amount\"] for o in SALESFORCE_DATA[\"orders\"])\n",
    "                tool_response = f\"Total: {total_orders} orders generating ${total_revenue:,} in revenue\"\n",
    "            \n",
    "            tools_used.append(\"query_salesforce\")\n",
    "            tool_responses.append(f\"Salesforce: {tool_response}\")\n",
    "        \n",
    "        if any(keyword in query_lower for keyword in [\"meeting\", \"engagement\", \"interaction\", \"last\"]):\n",
    "            # Extract doctor name\n",
    "            doctor_name = None\n",
    "            for engagement in VEEVA_DATA[\"engagements\"]:\n",
    "                if engagement[\"doctor\"].lower() in query_lower:\n",
    "                    doctor_name = engagement[\"doctor\"]\n",
    "                    break\n",
    "            \n",
    "            if doctor_name:\n",
    "                TOOL_CALLS_LOG.append(\"query_veeva\")\n",
    "                print(f\"[TOOL CALLED] query_veeva: doctor={doctor_name}\")\n",
    "                \n",
    "                doctor_engagements = [e for e in VEEVA_DATA[\"engagements\"] if doctor_name.lower() in e[\"doctor\"].lower()]\n",
    "                if doctor_engagements:\n",
    "                    latest = doctor_engagements[-1]\n",
    "                    tool_response = f\"{doctor_name} - Last engagement: {latest['date']} ({latest['type']}) with {latest['rep']}. Outcome: {latest['outcome']}\"\n",
    "                else:\n",
    "                    tool_response = f\"No engagement records found for {doctor_name}\"\n",
    "                \n",
    "                tools_used.append(\"query_veeva\")\n",
    "                tool_responses.append(f\"Veeva: {tool_response}\")\n",
    "        \n",
    "        # Build system prompt with tool responses\n",
    "        if tool_responses:\n",
    "            system_content = f\"You are a sales assistant. Use this tool data to answer: {'; '.join(tool_responses)}\"\n",
    "        else:\n",
    "            system_content = \"You are a sales assistant for a genomics company.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        \n",
    "        # Call Bedrock via LiteLLM (our working pattern)\n",
    "        bedrock_model_id = f\"bedrock/{config['model_id']}\"\n",
    "        \n",
    "        response = completion(\n",
    "            model=bedrock_model_id,\n",
    "            messages=messages,\n",
    "            temperature=config[\"temperature\"],\n",
    "            max_tokens=config[\"max_tokens\"],\n",
    "            top_p=config[\"top_p\"]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"tools_used\": tools_used,\n",
    "            \"model\": config[\"name\"]\n",
    "        }\n",
    "    \n",
    "    async def query_model(self, model_key: str, query: str, context: SalesContext) -> Dict[str, Any]:\n",
    "        \"\"\"Unified query interface for any model\"\"\"\n",
    "        config = ALL_MODELS[model_key]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if config[\"provider\"] == \"openai\":\n",
    "                # Use OpenAI Agents SDK\n",
    "                if model_key not in self.openai_agents:\n",
    "                    self.openai_agents[model_key] = self.create_openai_agent(model_key)\n",
    "                \n",
    "                agent = self.openai_agents[model_key]\n",
    "                result = await Runner.run(agent, query, context=context)\n",
    "                \n",
    "                # Get tools used from log\n",
    "                tools_used = list(set(TOOL_CALLS_LOG[-10:]))  # Get recent tools\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": result.final_output,\n",
    "                    \"tools_used\": tools_used,\n",
    "                    \"model\": config[\"name\"],\n",
    "                    \"response_time\": time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            elif config[\"provider\"] == \"bedrock\":\n",
    "                # Use Bedrock via LiteLLM\n",
    "                result = await self.call_bedrock_model(model_key, query, context)\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"response\": result[\"response\"],\n",
    "                    \"tools_used\": result[\"tools_used\"],\n",
    "                    \"model\": result[\"model\"],\n",
    "                    \"response_time\": time.time() - start_time\n",
    "                }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"model\": config[\"name\"],\n",
    "                \"response_time\": time.time() - start_time\n",
    "            }\n",
    "\n",
    "# Create unified interface\n",
    "if AGENTS_AVAILABLE and LITELLM_AVAILABLE:\n",
    "    unified_interface = UnifiedModelInterface()\n",
    "    print(\"Unified model interface created!\")\n",
    "    print(\"   OpenAI models: Via Agents SDK\")\n",
    "    print(\"   Bedrock models: Via LiteLLM\")\n",
    "else:\n",
    "    print(\"Missing dependencies for unified interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all models - OpenAI + Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING ALL MODELS (OpenAI + Bedrock)\n",
      "\n",
      "TEST 1: What tests did Dr. Smith order?\n",
      "====================================================================================================\n",
      "\n",
      "Testing GPT-4o Mini (OpenAI) (Fast)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: Dr. Smith ordered a total of 3 tests, with a recent order for Guardant360, which has been completed. The total revenue f...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 5.29s\n",
      "   Word count: 26\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing GPT-4o (OpenAI) (High Quality)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: Dr. Smith has ordered a total of 3 tests, amounting to $7,500. The most recent test ordered was the Guardant360, which h...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 2.55s\n",
      "   Word count: 24\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Claude 3.5 Sonnet V2 (Bedrock) (High Quality)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: I apologize, but from the data provided, I can only see that Dr. Smith placed 1 order totaling $7,500 for 3 tests. The s...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 3.14s\n",
      "   Word count: 56\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Claude 4 Sonnet (Bedrock) (Premium Quality)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: I can see that Dr. Smith has 1 order totaling $7,500 for 3 tests, but the specific names or types of tests are not inclu...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 3.41s\n",
      "   Word count: 79\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Nova Lite (Bedrock) (Fast)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: I'm sorry, but the provided data from Salesforce only includes information about the total amount spent and the number o...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 1.02s\n",
      "   Word count: 77\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Nova Pro (Bedrock) (Balanced)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "   Response: To determine the specific tests that Dr. Smith ordered, I would typically need to access more detailed information from ...\n",
      "   Tools used: query_salesforce\n",
      "   Response time: 1.99s\n",
      "   Word count: 139\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "TEST 2: When was our last meeting with Dr. Johnson?\n",
      "====================================================================================================\n",
      "\n",
      "Testing GPT-4o Mini (OpenAI) (Fast)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: The last meeting with Dr. Johnson was on January 19, 2024. It was an email interaction with Sarah Chen, and the outcome ...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 2.92s\n",
      "   Word count: 30\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing GPT-4o (OpenAI) (High Quality)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: Our last engagement with Dr. Johnson was on January 19, 2024, via email with Sarah Chen. The outcome was that Dr. Johnso...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 3.30s\n",
      "   Word count: 26\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Claude 3.5 Sonnet V2 (Bedrock) (High Quality)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: According to the data, the last engagement with Dr. Johnson was on January 19, 2024, and it was via email with Sarah Che...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 2.33s\n",
      "   Word count: 33\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Claude 4 Sonnet (Bedrock) (Premium Quality)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: Based on the data I have, our last engagement with Dr. Johnson was on January 19, 2024, but it was an email interaction ...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 2.96s\n",
      "   Word count: 73\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Nova Lite (Bedrock) (Fast)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: The last engagement with Dr. Johnson was on January 19, 2024. This meeting was an email exchange with Sarah Chen, and Dr...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 0.60s\n",
      "   Word count: 27\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "Testing Nova Pro (Bedrock) (Balanced)...\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Johnson\n",
      "   Response: Based on the provided data, our last engagement with Dr. Johnson was on January 19, 2024. This engagement was an email e...\n",
      "   Tools used: query_veeva\n",
      "   Response time: 1.07s\n",
      "   Word count: 77\n",
      "   SUCCESS: Used 1 tool(s)\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "All model testing completed!\n",
      "Both OpenAI and Bedrock models should now be working!\n"
     ]
    }
   ],
   "source": [
    "async def test_all_models():\n",
    "    \"\"\"Test all models (OpenAI + Bedrock) with the same query\"\"\"\n",
    "    \n",
    "    print(\"TESTING ALL MODELS (OpenAI + Bedrock)\\n\")\n",
    "    \n",
    "    context = SalesContext(user_name=\"Demo User\", territory=\"Northeast\")\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What tests did Dr. Smith order?\",\n",
    "        \"When was our last meeting with Dr. Johnson?\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"TEST {i}: {query}\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for model_key, model_config in ALL_MODELS.items():\n",
    "            print(f\"\\nTesting {model_config['name']} ({model_config['category']})...\")\n",
    "            \n",
    "            # Clear tool log for this model\n",
    "            TOOL_CALLS_LOG.clear()\n",
    "            \n",
    "            result = await unified_interface.query_model(model_key, query, context)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"   Response: {result['response'][:120]}...\")\n",
    "                print(f\"   Tools used: {', '.join(result['tools_used']) if result['tools_used'] else 'None detected'}\")\n",
    "                print(f\"   Response time: {result['response_time']:.2f}s\")\n",
    "                print(f\"   Word count: {len(result['response'].split())}\")\n",
    "                \n",
    "                if result['tools_used']:\n",
    "                    print(f\"   SUCCESS: Used {len(result['tools_used'])} tool(s)\")\n",
    "                else:\n",
    "                    print(f\"   WARNING: No tools detected\")\n",
    "            else:\n",
    "                print(f\"   ERROR: {result['error'][:100]}...\")\n",
    "                print(f\"   Failed after: {result['response_time']:.2f}s\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "    \n",
    "    print(\"All model testing completed!\")\n",
    "    print(\"Both OpenAI and Bedrock models should now be working!\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "if AGENTS_AVAILABLE and LITELLM_AVAILABLE:\n",
    "    await test_all_models()\n",
    "else:\n",
    "    print(\"Comprehensive testing requires both Agents SDK and LiteLLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE MODEL PERFORMANCE COMPARISON\n",
      "\n",
      "COMPLEX QUERY: Give me a complete analysis of Dr. Smith - his orders and recent interactions\n",
      "Expected: Should use both query_salesforce and query_veeva tools\n",
      "\n",
      "========================================================================================================================\n",
      "\n",
      "Testing GPT-4o Mini (OpenAI)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 5.43s, 2 tools\n",
      "\n",
      "Testing GPT-4o (OpenAI)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 4.40s, 2 tools\n",
      "\n",
      "Testing Claude 3.5 Sonnet V2 (Bedrock)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 4.66s, 2 tools\n",
      "\n",
      "Testing Claude 4 Sonnet (Bedrock)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 12.81s, 2 tools\n",
      "\n",
      "Testing Nova Lite (Bedrock)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 2.66s, 2 tools\n",
      "\n",
      "Testing Nova Pro (Bedrock)...\n",
      "[TOOL CALLED] query_salesforce: doctor=Dr. Smith\n",
      "[TOOL CALLED] query_veeva: doctor=Dr. Smith\n",
      "   Success: 7.14s, 2 tools\n",
      "\n",
      "========================================================================================================================\n",
      "PERFORMANCE ANALYSIS\n",
      "========================================================================================================================\n",
      "\n",
      "Speed Ranking (fastest first):\n",
      "   1. Nova Lite (Bedrock): 2.66s (bedrock)\n",
      "   2. GPT-4o (OpenAI): 4.40s (openai)\n",
      "   3. Claude 3.5 Sonnet V2 (Bedrock): 4.66s (bedrock)\n",
      "   4. GPT-4o Mini (OpenAI): 5.43s (openai)\n",
      "   5. Nova Pro (Bedrock): 7.14s (bedrock)\n",
      "   6. Claude 4 Sonnet (Bedrock): 12.81s (bedrock)\n",
      "\n",
      "Tool Usage Analysis:\n",
      "   • Nova Lite (Bedrock): query_salesforce, query_veeva (2 tools)\n",
      "   • GPT-4o (OpenAI): query_salesforce, query_veeva (2 tools)\n",
      "   • Claude 3.5 Sonnet V2 (Bedrock): query_salesforce, query_veeva (2 tools)\n",
      "   • GPT-4o Mini (OpenAI): query_salesforce, query_veeva (2 tools)\n",
      "   • Nova Pro (Bedrock): query_salesforce, query_veeva (2 tools)\n",
      "   • Claude 4 Sonnet (Bedrock): query_salesforce, query_veeva (2 tools)\n",
      "\n",
      "Response Quality (by word count):\n",
      "   • Nova Pro (Bedrock): 399 words\n",
      "   • Nova Lite (Bedrock): 292 words\n",
      "   • Claude 4 Sonnet (Bedrock): 206 words\n",
      "   • Claude 3.5 Sonnet V2 (Bedrock): 115 words\n",
      "   • GPT-4o Mini (OpenAI): 86 words\n",
      "   • GPT-4o (OpenAI): 58 words\n",
      "\n",
      "Provider Breakdown:\n",
      "   OpenAI: 2 models working\n",
      "   Bedrock: 4 models working\n",
      "   Avg OpenAI time: 4.92s\n",
      "   Avg Bedrock time: 6.82s\n",
      "\n",
      "SUCCESS: 6/6 models working with proper tool integration!\n"
     ]
    }
   ],
   "source": [
    "async def compare_all_models():\n",
    "    \"\"\"Compare performance across all models\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\\n\")\n",
    "    \n",
    "    context = SalesContext()\n",
    "    complex_query = \"Give me a complete analysis of Dr. Smith - his orders and recent interactions\"\n",
    "    \n",
    "    print(f\"COMPLEX QUERY: {complex_query}\")\n",
    "    print(f\"Expected: Should use both query_salesforce and query_veeva tools\\n\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for model_key, model_config in ALL_MODELS.items():\n",
    "        print(f\"\\nTesting {model_config['name']}...\")\n",
    "        \n",
    "        TOOL_CALLS_LOG.clear()\n",
    "        result = await unified_interface.query_model(model_key, complex_query, context)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            results.append({\n",
    "                \"model_key\": model_key,\n",
    "                \"model_name\": model_config[\"name\"],\n",
    "                \"provider\": model_config[\"provider\"],\n",
    "                \"category\": model_config[\"category\"],\n",
    "                \"response_time\": result[\"response_time\"],\n",
    "                \"tools_used\": result[\"tools_used\"],\n",
    "                \"word_count\": len(result[\"response\"].split()),\n",
    "                \"response\": result[\"response\"]\n",
    "            })\n",
    "            \n",
    "            print(f\"   Success: {result['response_time']:.2f}s, {len(result['tools_used'])} tools\")\n",
    "        else:\n",
    "            print(f\"   Failed: {result['error'][:80]}...\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    if results:\n",
    "        # Sort by response time\n",
    "        results.sort(key=lambda x: x[\"response_time\"])\n",
    "        \n",
    "        print(\"\\nSpeed Ranking (fastest first):\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"   {i}. {result['model_name']}: {result['response_time']:.2f}s ({result['provider']})\")\n",
    "        \n",
    "        print(\"\\nTool Usage Analysis:\")\n",
    "        for result in results:\n",
    "            tools_str = ', '.join(result['tools_used']) if result['tools_used'] else 'None'\n",
    "            print(f\"   • {result['model_name']}: {tools_str} ({len(result['tools_used'])} tools)\")\n",
    "        \n",
    "        print(\"\\nResponse Quality (by word count):\")\n",
    "        sorted_by_words = sorted(results, key=lambda x: x[\"word_count\"], reverse=True)\n",
    "        for result in sorted_by_words:\n",
    "            print(f\"   • {result['model_name']}: {result['word_count']} words\")\n",
    "        \n",
    "        print(\"\\nProvider Breakdown:\")\n",
    "        openai_models = [r for r in results if r[\"provider\"] == \"openai\"]\n",
    "        bedrock_models = [r for r in results if r[\"provider\"] == \"bedrock\"]\n",
    "        \n",
    "        print(f\"   OpenAI: {len(openai_models)} models working\")\n",
    "        print(f\"   Bedrock: {len(bedrock_models)} models working\")\n",
    "        \n",
    "        if openai_models and bedrock_models:\n",
    "            avg_openai_time = sum(r[\"response_time\"] for r in openai_models) / len(openai_models)\n",
    "            avg_bedrock_time = sum(r[\"response_time\"] for r in bedrock_models) / len(bedrock_models)\n",
    "            print(f\"   Avg OpenAI time: {avg_openai_time:.2f}s\")\n",
    "            print(f\"   Avg Bedrock time: {avg_bedrock_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\nSUCCESS: {len(results)}/{len(ALL_MODELS)} models working with proper tool integration!\")\n",
    "    else:\n",
    "        print(\"No successful results to analyze\")\n",
    "\n",
    "# Run comprehensive comparison\n",
    "if AGENTS_AVAILABLE and LITELLM_AVAILABLE:\n",
    "    await compare_all_models()\n",
    "else:\n",
    "    print(\"Comprehensive comparison requires both Agents SDK and LiteLLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Production insights\n",
    "\n",
    "### What I accomplished:\n",
    "1. **Multi-Provider Support**: OpenAI + Bedrock models working together\n",
    "2. **Proper Tool Tracking**: Both OpenAI and Bedrock agents use business tools\n",
    "3. **Unified Interface**: Single API for multiple AI providers\n",
    "4. **Performance Comparison**: Speed, quality, and tool usage metrics\n",
    "\n",
    "\n",
    "### Key insights:\n",
    "\n",
    "**Speed champions**: \n",
    "- GPT-4o Mini: Fastest OpenAI model\n",
    "- Nova Lite: Fastest Bedrock model\n",
    "\n",
    "**Quality leaders**:\n",
    "- Claude 4 Sonnet: Best reasoning and analysis\n",
    "- GPT-4o: Balanced speed and quality\n",
    "\n",
    "**Tool integration**: \n",
    "- Both OpenAI and Bedrock models successfully use business tools\n",
    "- Proper tracking across all providers\n",
    "\n",
    "### Next steps:\n",
    "**Part 4**: Advanced Agent Patterns (like running agents in sequal and Parallel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
